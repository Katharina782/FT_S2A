{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilation rate\n",
    "\n",
    "This script is used to better understand the dilated convolutional layers, especially the dilation rate and dilation rate multiplication factor in Basenji2. \n",
    "\n",
    "**Receptive field of 1D-convolution**\n",
    "$$r = (k-1)*d + 1$$ \n",
    "where $k$ is the kernel size and $d$ is the dilation rate.\n",
    "\n",
    "**Receptive field in bp**\n",
    "$$r_{bp} = r * b * 2 - crop * 2$$\n",
    "where $b$ is the bin size in bp (here 128) and $crop$ is the number of bp cropped from each side in final step (here 64)\n",
    "\n",
    "\n",
    "**Model architecture**\n",
    "\n",
    "* Importantly, the architectures used in Basenji1 and Basenji2 differ significantly, apart from the cross-species training. Basenji1 achieves a receptive field of 32,896 bp, while Basenji2 achieves a receptive field of 44,160 bp. \n",
    "* This is because Basenji1 uses only 6 layers of dilated convolution, while Basenji2 uses 11. \n",
    "* In my code there was a bug that caused the final dilation rate to be 58, resulting in a receptvie field of only 29,824 bp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receptive_field(kernel_size, dilation_rate):\n",
    "    return (kernel_size - 1) * dilation_rate + 1\n",
    "\n",
    "\n",
    "def convert_receptive_field_to_bp(receptive_field, bin_size=128, crop=64):\n",
    "    return receptive_field * bin_size * 2 - crop*2\n",
    "\n",
    "\n",
    "def receptive_field_bp(kernel_size, dilation_rate, bin_size=128, crop=64):\n",
    "    return convert_receptive_field_to_bp(receptive_field(kernel_size, dilation_rate), bin_size, crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basenji2 has a dilation rate of 86 in final dilated convolutioin layer, receptive field: 36096\n",
      "Basenji1 has a dilation rate of 64 in final dilated convolutioin layer, receptive field: 24832\n",
      "Before, the model had a bug and achieved a dilation rate of58, receptive field: 21760\n"
     ]
    }
   ],
   "source": [
    "print(f\"Basenji2 has a dilation rate of 86 in final dilated convolutioin layer, receptive field: {receptive_field_bp(3, 86)}\")\n",
    "print(f\"Basenji1 has a dilation rate of 64 in final dilated convolutioin layer, receptive field: {receptive_field_bp(3, 64)}\")\n",
    "print(f\"Before, the model had a bug and achieved a dilation rate of58, receptive field: {receptive_field_bp(3, 58)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bug was the order in which I use the initial dilation rate and multiply it with the multiplicative factor of 1.5.\n",
      "0: dil rate: 2, receptive field: 5, receptive field in bp: 576.0 in each direction\n",
      "1: dil rate: 2, receptive field: 5, receptive field in bp: 576.0 in each direction\n",
      "2: dil rate: 3, receptive field: 7, receptive field in bp: 832.0 in each direction\n",
      "3: dil rate: 5, receptive field: 11, receptive field in bp: 1344.0 in each direction\n",
      "4: dil rate: 8, receptive field: 17, receptive field in bp: 2112.0 in each direction\n",
      "5: dil rate: 11, receptive field: 23, receptive field in bp: 2880.0 in each direction\n",
      "6: dil rate: 17, receptive field: 35, receptive field in bp: 4416.0 in each direction\n",
      "7: dil rate: 26, receptive field: 53, receptive field in bp: 6720.0 in each direction\n",
      "8: dil rate: 38, receptive field: 77, receptive field in bp: 9792.0 in each direction\n",
      "9: dil rate: 58, receptive field: 117, receptive field in bp: 14912.0 in each direction\n",
      "10: dil rate: 86, receptive field: 173, receptive field in bp: 22080.0 in each direction\n",
      "Receptvie field of 44160 bp, 22080.0 in each direction\n",
      "so far:\n",
      "0: dil rate: 1, receptive field: 3, receptive field in bp: 320.0 in each direction\n",
      "1: dil rate: 2, receptive field: 5, receptive field in bp: 576.0 in each direction\n",
      "2: dil rate: 2, receptive field: 5, receptive field in bp: 576.0 in each direction\n",
      "3: dil rate: 3, receptive field: 7, receptive field in bp: 832.0 in each direction\n",
      "4: dil rate: 5, receptive field: 11, receptive field in bp: 1344.0 in each direction\n",
      "5: dil rate: 8, receptive field: 17, receptive field in bp: 2112.0 in each direction\n",
      "6: dil rate: 11, receptive field: 23, receptive field in bp: 2880.0 in each direction\n",
      "7: dil rate: 17, receptive field: 35, receptive field in bp: 4416.0 in each direction\n",
      "8: dil rate: 26, receptive field: 53, receptive field in bp: 6720.0 in each direction\n",
      "9: dil rate: 38, receptive field: 77, receptive field in bp: 9792.0 in each direction\n",
      "10: dil rate: 58, receptive field: 117, receptive field in bp: 14912.0 in each direction\n",
      "The buggy model had a receptive field of 29824bp\n"
     ]
    }
   ],
   "source": [
    "num_dil_layers = 11\n",
    "dil_rate = 1\n",
    "dil_mult = 1.5\n",
    "\n",
    "print(\"The bug was the order in which I use the initial dilation rate and multiply it with the multiplicative factor of 1.5.\")\n",
    "\n",
    "## first mulitply, then apply\n",
    "for i in range(num_dil_layers):\n",
    "    dil_rate *= dil_mult \n",
    "    dil_rate_new = int(np.round(dil_rate))\n",
    "    print(f\"{i}: dil rate: {dil_rate_new}, receptive field: {receptive_field(3, dil_rate_new)}, receptive field in bp: {receptive_field_bp(3, dil_rate_new) / 2} in each direction\")\n",
    "print(f\"Receptvie field of {receptive_field_bp(3, dil_rate_new)} bp, {receptive_field_bp(3, dil_rate_new) / 2} in each direction\")\n",
    "\n",
    "\n",
    "## In the buggy Basenji version, which did not achieve the desired receptive field and preformance the order was reversed:\n",
    "## first apply dilation rate, then multiply\n",
    "## This way I achieved only a dilation rate of 58, instead of 86!\n",
    "dil_rate = 1\n",
    "print(\"so far:\")\n",
    "for i in range(num_dil_layers):\n",
    "    dil_rate_new = int(np.round(dil_rate))\n",
    "    print(f\"{i}: dil rate: {dil_rate_new}, receptive field: {receptive_field(3, dil_rate_new)}, receptive field in bp: {receptive_field_bp(3, dil_rate_new) / 2} in each direction\")\n",
    "    dil_rate *= dil_mult \n",
    "print(f\"The buggy model had a receptive field of {receptive_field_bp(3, dil_rate_new)}bp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedLayers(nn.Module):\n",
    "    def __init__(self, num_dilated_conv:int, input_size:int, channel_init:int, kernel_size=3, dilation_rate_init=1, rate_mult=1.5, dropout_rate=0.3, bn_momentum=0.1):\n",
    "        super(DilatedLayers, self).__init__()\n",
    "        self.dilated_layers = nn.ModuleList()\n",
    "        self.layer_dimensions = {}\n",
    "        self.dilation_rate = dilation_rate_init\n",
    "        self.rate_mult = rate_mult\n",
    "        self.kernel_size = kernel_size\n",
    "        print(dilation_rate_init, rate_mult)\n",
    "        for layer in range(num_dilated_conv):\n",
    "            self.dilation_rate *= self.rate_mult\n",
    "            self.dilated_layers.append(\n",
    "                nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_size,\n",
    "                          out_channels=channel_init,\n",
    "                          kernel_size=self.kernel_size,\n",
    "                          dilation=int(np.round(self.dilation_rate)),\n",
    "                          padding=\"same\"),\n",
    "                nn.BatchNorm1d(channel_init, momentum=bn_momentum),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(in_channels=channel_init,\n",
    "                        out_channels=input_size,\n",
    "                        kernel_size=1, \n",
    "                        padding=\"same\"),\n",
    "                nn.BatchNorm1d(input_size, momentum=bn_momentum),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.GELU()\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5\n"
     ]
    }
   ],
   "source": [
    "dil_block = DilatedLayers(num_dilated_conv=11, input_size=768, channel_init=768//2, kernel_size=3, dilation_rate_init=1, rate_mult=1.5, dropout_rate=0.3, bn_momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DilatedLayers(\n",
       "  (dilated_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(3,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(5,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(11,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(17,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(26,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(38,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(58,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=same, dilation=(86,))\n",
       "      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv1d(384, 768, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      (4): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dil_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basenji_architecture import * \n",
    "\n",
    "num_dilated_conv = 11\n",
    "num_conv = 6\n",
    "conv_target_channels = 768\n",
    "dilation_rate_init = 1\n",
    "bn_momentum = .9\n",
    "dilation_rate_mult = 1.5\n",
    "experiments_human = 5313\n",
    "experimental_tracks = 37\n",
    "\n",
    "model = BasenjiModel( \n",
    "                 n_conv_layers=num_conv,\n",
    "                 n_dilated_conv_layers=num_dilated_conv, \n",
    "                 conv_target_channels=conv_target_channels,\n",
    "                 bn_momentum=bn_momentum,\n",
    "                 dilation_rate_init=dilation_rate_init, \n",
    "                 dilation_rate_mult=dilation_rate_mult, \n",
    "                 human_tracks=experiments_human, \n",
    "                 mouse_tracks=experimental_tracks).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.randint(low=0, high=1, size=(1, 131072, 4), dtype=torch.float32)\n",
    "seq.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated conv input: torch.Size([1, 288, 65536])\n",
      "Dilated conv output: torch.Size([1, 339, 32768])\n",
      "Dilated conv input: torch.Size([1, 339, 32768])\n",
      "Dilated conv output: torch.Size([1, 399, 16384])\n",
      "Dilated conv input: torch.Size([1, 399, 16384])\n",
      "Dilated conv output: torch.Size([1, 470, 8192])\n",
      "Dilated conv input: torch.Size([1, 470, 8192])\n",
      "Dilated conv output: torch.Size([1, 553, 4096])\n",
      "Dilated conv input: torch.Size([1, 553, 4096])\n",
      "Dilated conv output: torch.Size([1, 651, 2048])\n",
      "Dilated conv input: torch.Size([1, 651, 2048])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n",
      "Dilated conv input: torch.Size([1, 768, 1024])\n",
      "Dilated conv output: torch.Size([1, 768, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5395, 1.2559, 0.1543,  ..., 0.5421, 0.2309, 0.6245],\n",
       "         [0.2689, 0.8649, 0.8859,  ..., 0.3580, 0.5003, 0.1780],\n",
       "         [0.0548, 3.5790, 0.4456,  ..., 0.5636, 0.0936, 0.8739],\n",
       "         ...,\n",
       "         [0.2198, 1.8460, 0.2736,  ..., 0.6718, 1.0371, 0.5696],\n",
       "         [0.7955, 1.5577, 0.1754,  ..., 0.5003, 0.2609, 1.3662],\n",
       "         [0.6656, 0.7462, 0.1997,  ..., 1.3622, 0.0172, 0.0856]]],\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(seq, \"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution layers\n",
    "\n",
    "The Conv stem creates a tensor of shape `torch.Size([1, 288, 65536])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_0</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>layer_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># channels</th>\n",
       "      <td>339</td>\n",
       "      <td>399</td>\n",
       "      <td>470</td>\n",
       "      <td>553</td>\n",
       "      <td>651</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence length</th>\n",
       "      <td>32768</td>\n",
       "      <td>16384</td>\n",
       "      <td>8192</td>\n",
       "      <td>4096</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 layer_0  layer_1  layer_2  layer_3  layer_4  layer_5\n",
       "index                                                                \n",
       "x                      1        1        1        1        1        1\n",
       "# channels           339      399      470      553      651      768\n",
       "sequence length    32768    16384     8192     4096     2048     1024"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers_df = pd.DataFrame(model.conv_layers.layer_dimensions)\n",
    "conv_layers_df[\"index\"] = [\"x\", \"# channels\", \"sequence length\"]\n",
    "conv_layers_df = conv_layers_df.set_index(\"index\")\n",
    "conv_layers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilated convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_0</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>layer_5</th>\n",
       "      <th>layer_6</th>\n",
       "      <th>layer_7</th>\n",
       "      <th>layer_8</th>\n",
       "      <th>layer_9</th>\n",
       "      <th>layer_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># channels</th>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequene length</th>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dilation rate</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>58</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernel size</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                layer_0  layer_1  layer_2  layer_3  layer_4  layer_5  layer_6   \n",
       "index                                                                           \n",
       "# channels          768      768      768      768      768      768      768  \\\n",
       "sequene length     1024     1024     1024     1024     1024     1024     1024   \n",
       "dilation rate         2        2        3        5        8       11       17   \n",
       "kernel size           3        3        3        3        3        3        3   \n",
       "\n",
       "                layer_7  layer_8  layer_9  layer_10  \n",
       "index                                                \n",
       "# channels          768      768      768       768  \n",
       "sequene length     1024     1024     1024      1024  \n",
       "dilation rate        26       38       58        86  \n",
       "kernel size           3        3        3         3  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dil_layers_df = pd.DataFrame(model.dilated_layers.layer_dimensions)\n",
    "dil_layers_df[\"index\"] = [\"# channels\", \"sequene length\", \"dilation rate\", \"kernel size\"]\n",
    "dil_layers_df = dil_layers_df.set_index(\"index\")\n",
    "dil_layers_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
